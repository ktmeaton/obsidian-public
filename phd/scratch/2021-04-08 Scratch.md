# 2021-04-08

Today I start by troubleshooting [[TreeTime]] errors. I chose to print the variables ```new_xmin``` and ```new_xmax``` from the ```multiply``` function which was fruitful. Here are some observed ranges:

```yaml
joint:
  - new_xmin: 4.583546061914727e-07
  - new_xmax: 5.314118684385571
marginal:
  - new_xmin: 4.499083393097417e-07
  - new_xmax: 0.0956283987375348
marginal-error:
  - 7.377926997028264e-07
  - 6.92651798532656e-07
```

Here is the code to examine:
```python
new_xmin = np.max([k.xmin for k in dists])
new_xmax = np.min([k.xmax for k in dists])

x_vals = np.unique(np.concatenate([k.x for k in dists]))
x_vals = x_vals[(x_vals>new_xmin-TINY_NUMBER)&(x_vals<new_xmax+TINY_NUMBER)]
# (7.377926997028264e-07 - 1e-12) = 7.377916997028263e-07
# (6.92651798532656e-07 + 1e-12)  = 6.926527985326561e-07
y_vals = np.sum([k.__call__(x_vals) for k in dists], axis=0)
peak = y_vals.min()
```

The next test is to print the len of x_vals and y_vals while running:
```yaml
joint:
  - dists: 1
    - x_vals_pre: 268
	- x_vals_post: 268
	- y_vals: 268
  - dists: 2
    - x_vals_pre: 386
	- x_vals_post: 195
	- y_vals: 195	
marginal-error:	
  - dists: 2
	- x_vals_pre: 455
	- x_vals_post: 0
	- y_vals: 0	
```

Also, I wonder if I should change in config:
```python
MIN_BRANCH_LENGTH = 1e-3 # fraction of length 'one_mutation' that is used as lower cut-off for branch lengths in GTR
```

I'm starting to get other weird function errors so I should wipe my copy. I created a fresh fork of [[TreeTime]]. I know this error occurs:
  - In the final round of optimization with [[Marginal]] likelihoods.
  - When the distributions of dates and likelihoods are being multiplied.
  - When a distribution has a very small range, and the xmax is very small.
  - The x_vals and y_vals arrays are always of the same length. So if y_vals is empty, it's because x_vals is empty.
  
 This appears to be the singular problem line:
 ```python
 x_vals = x_vals[(x_vals>new_xmin-TINY_NUMBER)&(x_vals<new_xmax+TINY_NUMBER)]
 ```
 
 1. Is it a numerical instability in comparing extremely small numbers?
```python
TINY_NUMBER = 1e-12	
BIG_NUMBER = 1e10

# Normal
new_xmin = 4.499083393097417e-07 
new_xmax = 5.314118684385571
x = [4.499083393097417e-07, 4.499083393097417e-06]
x_vals = np.unique(np.concatenate([x]))
x_vals = x_vals[(x_vals>new_xmin-TINY_NUMBER)&(x_vals<new_xmax+TINY_NUMBER)]
y_vals = np.array([-68000, 67000])
peak = y_vals.min()
ind = (y_vals-peak)<BIG_NUMBER/1000
n_points = ind.sum()


# Error
new_xmin = 7.377926997028264e-07
new_xmax = 6.92651798532656e-07	
x = [7.377926997028264e-07, 8.377926997028264e-07]

```

## Emails

### [[Sebastian Duchene]] and [[@Leo Featherstone]] update:

> Hi Everyone,
>
> I had 5 immediate 'action' items for myself from last meeting:
> 1. Review which samples can have fixed ages.
> 1. Create dataset (alignment, metadata) of all samples with geo and temporal information.
> 1. Create dataset (alignment, metadata) of all 20th century samples.
> 1. Leo requested dates to be nicely formatted :) Do you want that file in the format:
>   ```text
>   sample1Accession	5
>   sample2Accession	46
>   sample3Accession	1400
>   ```
>   Where the second column is dates before present (ie. 2021) and is the mean when there is a range.
>   
>   5. Experiment with alternative visualizations for geographic data.
>   
>   Could I get your opinions on this [arc diagram](https://rawcdn.githack.com/ktmeaton/plague-phylogeography/337baaf80e5e1f356737e46bb0bf8afb2f164fd8/workflow/scripts/arc_diagram.html) that I made? 
>   
>Cheers,
>Kat
### Full Dataset

- Everything data (no geo or temporal uncertainty)

### Limited Dataset
- 20th century dataset with sampling priors
- Review what samples we can fix the ages.

## [[BEAST2 Experiment]]

I really like the analysis of [[Dellicour 2020 Phylodynamic Workflow Rapidly|Dellicour et al. (2020)]].